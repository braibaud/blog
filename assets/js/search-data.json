{
  
    
        "post0": {
            "title": "Check whether TensorFlow runs on GPU",
            "content": "import tensorflow as tf . if tf.test.gpu_device_name(): print(&#39;Default GPU Device: {}&#39;.format(tf.test.gpu_device_name())) else: print(&#39;Please install GPU version of TF&#39;) . Please install GPU version of TF .",
            "url": "https://braibaud.github.io/blog/python/machine%20learning/tensorflow/gpu/2021/06/01/Check-whether-TensorFlow-runs-on-GPU.html",
            "relUrl": "/python/machine%20learning/tensorflow/gpu/2021/06/01/Check-whether-TensorFlow-runs-on-GPU.html",
            "date": " • Jun 1, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "LSTM timeseries forecasting with Keras Tuner",
            "content": "About . This project is a demonstration of some of capabilities of Keras Tuner. This project is an attempt to use an LSTM based neural network (RNN) to forecast timeseries data. . The required libraries . Import the must-have libraries: . import numpy as np import pandas as pd import datetime as dt . Import the elements required from the scikit-learn library: . import sklearn as sk import sklearn.preprocessing as skp import sklearn.model_selection as skms import sklearn.pipeline as skpl import sklearn.decomposition as skd import sklearn.linear_model as sklm import sklearn.dummy as sky import sklearn.metrics as skme . Enables defining partial functions: . from functools import partial . Import the keras elements from the tensorflow library: . import tensorflow as tf from tensorflow import keras as k from tensorflow.keras import backend as kb from tensorflow.keras import callbacks as kc from tensorflow.keras import models as km from tensorflow.keras import layers as kl from tensorflow.keras import regularizers as kr from tensorflow.keras import optimizers as ko from tensorflow.keras import utils as ku . Import the keras-tuner library as we&#39;ll use it to tune hyperparameters: . import kerastuner as kt from kerastuner import tuners as ktt . Import matplotlib and set the default magic: . import matplotlib as mat from matplotlib import pyplot as plt import pylab as pyl import seaborn as sns %matplotlib inline %config InlineBackend.figure_format = &#39;retina&#39; . Import the mlviz library used to plot time-series visualizations: . from mlviz.timeseries import visualizationhelpers as mwvh from mlviz.utilities import graphichelpers as mwgh . The project parameters . Reset the predefined matplotlib style: . mwgh.GraphicsStatics.initialize_matplotlib_styles() . Here is the color palette of this project: . sns.palplot(mwgh.GraphicsStatics.g_palette) . Let&#39;s capture all the usefull project&#39;s parameters in a dictionary: . params = {} params[&#39;project_date&#39;] = &#39;2021-05-31&#39; params[&#39;project_name&#39;] = &#39;LSTM timeseries forecasting with Keras Tuner-DATA&#39; params[&#39;experiment_name&#39;] = &#39;{0}-{1}&#39;.format(params[&#39;project_date&#39;], params[&#39;project_name&#39;]) params[&#39;data_frequency&#39;] = &#39;W-SAT&#39; params[&#39;features&#39;] = [&#39;y&#39;] params[&#39;input_size&#39;] = 52 # N weeks of data to input into the network params[&#39;output_size&#39;] = 13 # N weeks of data to output from the network params[&#39;testing_size&#39;] = 52 # N weeks of data to keep for testing the model params[&#39;lag_size&#39;] = [13, 26, 39] # Lagged data series injected into the network params[&#39;hyperband_iterations&#39;] = 3 params[&#39;max_epochs&#39;] = 100 params[&#39;patience&#39;] = 20 params[&#39;batch_size&#39;] = 16 . The timeseries data . The input data is available in a csv file named 2021-05-31-LSTM timeseries forecasting with Keras Tuner-DATA.csv located in the data folder. It has got 2 columns date containing the date of event and value holding the value of the source. We&#39;ll rename these 2 columns as ds and y for convenience. Let&#39;s load the csv file using the pandas library and have a look at the data. . df = pd.read_csv( filepath_or_buffer=&#39;../assets/data/{0}.csv&#39;.format( params[&#39;experiment_name&#39;]), sep=&#39;;&#39;) df.rename( columns = { &#39;date&#39;: &#39;index&#39;, &#39;value&#39;: &#39;y&#39; }, inplace=True) df[&#39;index&#39;] = pd.to_datetime( arg=df[&#39;index&#39;], dayfirst=True) df.sort_values( by=&#39;index&#39;, ascending=True, inplace=True) df.set_index( keys=&#39;index&#39;, inplace=True) df = df.asfreq( freq=params[&#39;data_frequency&#39;]) df[&#39;ds&#39;] = df.index print(&#39;df.shape = {0}&#39;.format(df.shape)) df.tail(5) . . df.shape = (625, 2) . y ds . index . 2019-09-28 5547 | 2019-09-28 | . 2019-10-05 6459 | 2019-10-05 | . 2019-10-12 5838 | 2019-10-12 | . 2019-10-19 5894 | 2019-10-19 | . 2019-10-26 7925 | 2019-10-26 | . Prepare data for the network . It is time to prepare the dataset to feed into the LSTM network. We&#39;ll use 4 features as input: the current data point as well as 3 additional data points from respectively 13, 26 and 39 weeks before the current datapoint. . . It&#39;s easy to calculate the width of a sample: . sample_width = max(params[&#39;lag_size&#39;]) + params[&#39;input_size&#39;] + params[&#39;output_size&#39;] print(&#39;sample_width: {0}&#39;.format(sample_width)) . sample_width: 104 . To avoid any overlap between the training and the testing data set, we&#39;ll first split the dataframes, keeping params[&#39;testing_size&#39;] samples for testing our model. We need to make sure that no data point used for training is also used for testing our model. . threshold_date = pd.to_datetime(df.index[df.shape[0] - (sample_width + params[&#39;testing_size&#39;])]) print(&#39;Cutoff date for training/testing split is {0}&#39;.format(threshold_date.strftime(&#39;%d/%m/%Y&#39;))) . Cutoff date for training/testing split is 05/11/2016 . Let&#39;s cut the dataframe at the right date: . test_mask = (df[&#39;ds&#39;] &gt; threshold_date) df_train = df[~test_mask] df_test = df[test_mask] print(&#39;df_train.shape = {0}&#39;.format(df_train.shape)) print(&#39;df_test.shape = {0}&#39;.format(df_test.shape)) . df_train.shape = (470, 2) df_test.shape = (155, 2) . The prepare_data funtion will take care of doing exactly this: . def prepare_data(data, lag_data, cols_in, steps_in, cols_out, steps_out, scaler_in=None, scaler_out=None): df = data.copy() cols_in_original = [col for col in cols_in] cols_in_processed = [col for col in cols_in] steps_lag = 0 if lag_data is not None and len(lag_data) &gt; 0: steps_lag = max(lag_data) for col in cols_in_original: for i, lag in enumerate(lag_data): lag_col = &#39;{0}_{1}&#39;.format(col, lag) df[lag_col] = df[col].shift(lag) cols_in_processed.append(lag_col) samples = df.shape[0] - (steps_in + steps_out + steps_lag) + 1 if samples &lt; 1: raise ValueError(&#39;not enough data to produce 1 sample.&#39;) index = list(df.index) cols_in_indices = {name: i for i, name in enumerate(cols_in_processed)} cols_out_indices = {name: i for i, name in enumerate(cols_out)} df.reset_index(inplace=True) X_input_scaled = None if scaler_in is None: scaler_in = skpl.Pipeline([ (&#39;std&#39;, skp.StandardScaler()), (&#39;minmax&#39;, skp.MinMaxScaler(feature_range=(-1, 1)))]) X_input_scaled = scaler_in.fit_transform(df[cols_in_processed].values) else: X_input_scaled = scaler_in.transform(df[cols_in_processed].values) y_output_scaled = None if scaler_out is None: scaler_out = skpl.Pipeline([ (&#39;std&#39;, skp.StandardScaler()), (&#39;minmax&#39;, skp.MinMaxScaler(feature_range=(-1, 1)))]) y_output_scaled = scaler_out.fit_transform(df[cols_out].values) else: y_output_scaled = scaler_out.transform(df[cols_out].values) X = [] y = [] for sample in range(samples): for step_in in range(steps_in): for col_in in range(len(cols_in_processed)): X.append(X_input_scaled[sample+steps_lag+step_in, col_in]) for step_out in range(steps_out): for col_out in range(len(cols_out)): y.append(y_output_scaled[sample+steps_lag+steps_in+step_out, col_out]) X = np.array(X).reshape(samples, steps_in, len(cols_in_processed)) y = np.array(y).reshape(samples, steps_out, len(cols_out)) return X, y, index, scaler_in, scaler_out, cols_in_indices, cols_out_indices . Every intput feature (passed via the function parameter cols_in) is going to be rescalled using a scikit-learn pipeline containing first a StandardScaler and then a MinMaxScaler in order to end up with a feature range of [-1, +1] required by neural networks. . X_train, y_train, index_train, scaler_in, scaler_out, cols_in_indices_train, cols_out_indices_train = prepare_data( data=df_train, lag_data=params[&#39;lag_size&#39;], cols_in=params[&#39;features&#39;], steps_in=params[&#39;input_size&#39;], cols_out=params[&#39;features&#39;], steps_out=params[&#39;output_size&#39;]) print(&#39;X_train.shape: {0}&#39;.format(X_train.shape)) print(&#39;y_train.shape: {0}&#39;.format(y_train.shape)) . X_train.shape: (367, 52, 4) y_train.shape: (367, 13, 1) . To prepare the testing data, we need to reuse both input (variable scaler_in) and output (variable scaler_out) pipelines in order to keep data scaled in the same way. . X_test, y_test, index_test, _, _, cols_in_indices_test, cols_out_indices_test = prepare_data( data=df_test, lag_data=params[&#39;lag_size&#39;], cols_in=params[&#39;features&#39;], steps_in=params[&#39;input_size&#39;], cols_out=params[&#39;features&#39;], steps_out=params[&#39;output_size&#39;], scaler_in=scaler_in, scaler_out=scaler_out) print(&#39;X_test.shape: {0}&#39;.format(X_test.shape)) print(&#39;y_test.shape: {0}&#39;.format(y_test.shape)) . X_test.shape: (52, 52, 4) y_test.shape: (52, 13, 1) . Data visualization . Let&#39;s have a look at the visual representation of the timeseries data: . mwvh.plot_time_series( title=&#39;timeseries visualization&#39;, subtitle=&#39;data preparation&#39;, name=(&#39;Input - Data Visualization{0}&#39; + &#39;Training - {1} weeks{0}&#39; + &#39;Testing - {2} weeks&#39;).format( &#39; n&#39;, df_train.shape[0], df_test.shape[0]), training=df_train, testing=df_test, ylabel=&#39;feature value (y)&#39;, split=threshold_date) . Build the model . Hyperparameters space definition . We&#39;ll use RMSE as our loss function to optimize (it is required to be defined as a function that can be compiled by TensorFlow): . def rmse(y_true, y_pred): return tf.cast( tf.sqrt( tf.reduce_mean( tf.square( tf.subtract( y_pred, y_true)))), dtype=tf.float32) . First, we define a model-building function. It takes an argument hp from which Keras-Tuner can sample hyperparameters: . def build_model_hp(hp, loss_fn, metrics_fn, steps_in, steps_out, n_features): # define model model = km.Sequential() model.add( kl.Bidirectional( layer=kl.LSTM( name=&#39;lstm_1&#39;, kernel_regularizer=kr.l2( l=hp.Float( name=&#39;lstm_1_kernel_regularizer_l2_alpha&#39;, min_value=1e-5, max_value=1e-3, sampling=&#39;log&#39;)), bias_regularizer=kr.l2( l=hp.Float( name=&#39;lstm_1_bias_regularizer_l2_alpha&#39;, min_value=1e-5, max_value=1e-3, sampling=&#39;log&#39;)), units=hp.Int( name=&#39;lstm_1_units&#39;, min_value=64, max_value=128, step=4), input_shape=(steps_in, n_features), dropout=hp.Float( name=&#39;lstm_1_dropout&#39;, min_value=0.3, max_value=0.5, step=0.1), return_sequences=False))) model.add(kl.RepeatVector(n=steps_out)) model.add( kl.Bidirectional( layer=kl.LSTM( name=&#39;lstm_2&#39;, kernel_regularizer=kr.l2( l=hp.Float( name=&#39;lstm_2_kernel_regularizer_l2_alpha&#39;, min_value=1e-5, max_value=1e-3, sampling=&#39;log&#39;)), bias_regularizer=kr.l2( l=hp.Float( name=&#39;lstm_2_bias_regularizer_l2_alpha&#39;, min_value=1e-5, max_value=1e-3, sampling=&#39;log&#39;)), units=hp.Int( name=&#39;lstm_2_units&#39;, min_value=64, max_value=128, step=4), dropout=hp.Float( name=&#39;lstm_2_dropout&#39;, min_value=0.3, max_value=0.5, step=0.1), return_sequences=True))) dense_dropout = hp.Float( name=&#39;dense_dropout&#39;, min_value=0.3, max_value=0.5, step=0.1) for n_layer in reversed(range(4)): layer_size = (1 + n_layer) ** 2 model.add( kl.TimeDistributed( layer=kl.Dense( name=&#39;dense_{0}&#39;.format(n_layer), activation=&#39;tanh&#39;, kernel_regularizer=kr.l2( l=hp.Float( name=&#39;dense_{0}_kernel_regularizer_l2_alpha&#39;.format(n_layer), min_value=1e-5, max_value=1e-3, sampling=&#39;log&#39;)), bias_regularizer=kr.l2( l=hp.Float( name=&#39;dense_{0}_bias_regularizer_l2_alpha&#39;.format(n_layer), min_value=1e-5, max_value=1e-3, sampling=&#39;log&#39;)), units=hp.Int( name=&#39;dense_{0}_units&#39;.format(n_layer), min_value=8*layer_size, max_value=16*layer_size, default=8*layer_size, step=2*layer_size)))) model.add( kl.Dropout( name=&#39;dense_{0}_dropout&#39;.format(n_layer), rate=dense_dropout)) model.add( kl.TimeDistributed( layer=kl.Dense( name=&#39;dense_output&#39;, kernel_regularizer=kr.l2( l=hp.Float( name=&#39;dense_output_kernel_regularizer_l2_alpha&#39;, min_value=1e-5, max_value=1e-3, sampling=&#39;log&#39;)), bias_regularizer=kr.l2( l=hp.Float( name=&#39;dense_output_bias_regularizer_l2_alpha&#39;, min_value=1e-5, max_value=1e-3, sampling=&#39;log&#39;)), activation=&#39;linear&#39;, units=1))) model.compile( loss=loss_fn, metrics=metrics_fn, optimizer=ko.Adam( learning_rate=hp.Float( name=&#39;learning_rate&#39;, min_value=1e-5, max_value=1e-3, sampling=&#39;log&#39;))) return model . It is required to define a partial wrapping the build-function to ensure the signature matches the Keras-Tuner expectations: . build_model = partial( build_model_hp, loss_fn=rmse, metrics_fn=None, steps_in=params[&#39;input_size&#39;], steps_out=params[&#39;output_size&#39;], n_features=len(params[&#39;features&#39;])) . We can create an Hyperband tuner to do the hyperparameter search. The main objective is to minimize the validation loss. . tuner = ktt.Hyperband( build_model, project_name=params[&#39;experiment_name&#39;], directory=&#39;tuner&#39;, objective=&#39;val_loss&#39;, hyperband_iterations=params[&#39;hyperband_iterations&#39;], max_epochs=params[&#39;max_epochs&#39;]) . INFO:tensorflow:Reloading Oracle from existing project tuner 2021-05-31-LSTM timeseries forecasting with Keras Tuner-DATA oracle.json . We need this class to clear cell output after each trial: . from IPython import display as ids class ClearTrainingOutput(kc.Callback): def on_train_end(*args, **kwargs): ids.clear_output(wait=True) . We can performs a search for best hyperparameters configuration: . tuner.search( x=X_train, y=y_train, shuffle=True, batch_size=params[&#39;batch_size&#39;], validation_data=(X_test, y_test), epochs=params[&#39;max_epochs&#39;], callbacks=[ ClearTrainingOutput(), kc.EarlyStopping( monitor=&#39;val_loss&#39;, patience=params[&#39;patience&#39;], verbose=1, mode=&#39;min&#39;, restore_best_weights=True), kc.TerminateOnNaN() ], verbose=2) . Trial 21 Complete [00h 00m 25s] val_loss: 0.32179614901542664 Best val_loss So Far: 0.32179614901542664 Total elapsed time: 00h 09m 36s Search: Running Trial #22 Hyperparameter |Value |Best Value So Far lstm_1_kernel_r...|0.00017755 |7.8621e-05 lstm_1_bias_reg...|3.5029e-05 |1.6744e-05 lstm_1_units |100 |76 lstm_1_dropout |0.4 |0.3 lstm_2_kernel_r...|0.00011003 |0.00018485 lstm_2_bias_reg...|0.00087325 |1.1831e-05 lstm_2_units |68 |80 lstm_2_dropout |0.3 |0.3 dense_dropout |0.5 |0.4 dense_3_kernel_...|0.00015975 |2.1419e-05 dense_3_bias_re...|8.3077e-05 |0.00053076 dense_3_units |160 |192 dense_2_kernel_...|0.00018244 |2.7891e-05 dense_2_bias_re...|3.6558e-05 |6.1412e-05 dense_2_units |108 |90 dense_1_kernel_...|1.9305e-05 |0.00049548 dense_1_bias_re...|1.1297e-05 |0.00048274 dense_1_units |48 |40 dense_0_kernel_...|2.1682e-05 |0.00022316 dense_0_bias_re...|2.5726e-05 |0.00016235 dense_0_units |14 |16 dense_output_ke...|3.7152e-05 |0.0002189 dense_output_bi...|1.0035e-05 |0.00019091 learning_rate |1.2916e-05 |2.7611e-05 tuner/epochs |2 |2 tuner/initial_e...|0 |0 tuner/bracket |4 |4 tuner/round |0 |0 Epoch 1/2 . InvalidArgumentError Traceback (most recent call last) ~ anaconda3 envs blog-env lib site-packages tensorflow python framework ops.py in get_attr(self, name) 2485 with c_api_util.tf_buffer() as buf: -&gt; 2486 pywrap_tf_session.TF_OperationGetAttrValueProto(self._c_op, name, buf) 2487 data = pywrap_tf_session.TF_GetBuffer(buf) InvalidArgumentError: Operation &#39;split_1&#39; has no attr named &#39;_XlaCompile&#39;. During handling of the above exception, another exception occurred: ValueError Traceback (most recent call last) ~ anaconda3 envs blog-env lib site-packages tensorflow python ops gradients_util.py in _MaybeCompile(scope, op, func, grad_fn) 330 try: --&gt; 331 xla_compile = op.get_attr(&#34;_XlaCompile&#34;) 332 xla_separate_compiled_gradients = op.get_attr( ~ anaconda3 envs blog-env lib site-packages tensorflow python framework ops.py in get_attr(self, name) 2489 # Convert to ValueError for backwards compatibility. -&gt; 2490 raise ValueError(str(e)) 2491 x = attr_value_pb2.AttrValue() ValueError: Operation &#39;split_1&#39; has no attr named &#39;_XlaCompile&#39;. During handling of the above exception, another exception occurred: KeyboardInterrupt Traceback (most recent call last) &lt;ipython-input-26-1e077e1cd88c&gt; in &lt;module&gt; 16 kc.TerminateOnNaN() 17 ], &gt; 18 verbose=2) ~ anaconda3 envs blog-env lib site-packages kerastuner engine base_tuner.py in search(self, *fit_args, **fit_kwargs) 129 130 self.on_trial_begin(trial) --&gt; 131 self.run_trial(trial, *fit_args, **fit_kwargs) 132 self.on_trial_end(trial) 133 self.on_search_end() ~ anaconda3 envs blog-env lib site-packages kerastuner tuners hyperband.py in run_trial(self, trial, *fit_args, **fit_kwargs) 352 fit_kwargs[&#39;epochs&#39;] = hp.values[&#39;tuner/epochs&#39;] 353 fit_kwargs[&#39;initial_epoch&#39;] = hp.values[&#39;tuner/initial_epoch&#39;] --&gt; 354 super(Hyperband, self).run_trial(trial, *fit_args, **fit_kwargs) 355 356 def _build_model(self, hp): ~ anaconda3 envs blog-env lib site-packages kerastuner engine multi_execution_tuner.py in run_trial(self, trial, *fit_args, **fit_kwargs) 94 copied_fit_kwargs[&#39;callbacks&#39;] = callbacks 95 &gt; 96 history = self._build_and_fit_model(trial, fit_args, copied_fit_kwargs) 97 for metric, epoch_values in history.history.items(): 98 if self.oracle.objective.direction == &#39;min&#39;: ~ anaconda3 envs blog-env lib site-packages kerastuner engine tuner.py in _build_and_fit_model(self, trial, fit_args, fit_kwargs) 139 &#34;&#34;&#34; 140 model = self.hypermodel.build(trial.hyperparameters) --&gt; 141 return model.fit(*fit_args, **fit_kwargs) 142 143 def run_trial(self, trial, *fit_args, **fit_kwargs): ~ anaconda3 envs blog-env lib site-packages tensorflow python keras engine training.py in _method_wrapper(self, *args, **kwargs) 106 def _method_wrapper(self, *args, **kwargs): 107 if not self._in_multi_worker_mode(): # pylint: disable=protected-access --&gt; 108 return method(self, *args, **kwargs) 109 110 # Running inside `run_distribute_coordinator` already. ~ anaconda3 envs blog-env lib site-packages tensorflow python keras engine training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing) 1096 batch_size=batch_size): 1097 callbacks.on_train_batch_begin(step) -&gt; 1098 tmp_logs = train_function(iterator) 1099 if data_handler.should_sync: 1100 context.async_wait() ~ anaconda3 envs blog-env lib site-packages tensorflow python eager def_function.py in __call__(self, *args, **kwds) 778 else: 779 compiler = &#34;nonXla&#34; --&gt; 780 result = self._call(*args, **kwds) 781 782 new_tracing_count = self._get_tracing_count() ~ anaconda3 envs blog-env lib site-packages tensorflow python eager def_function.py in _call(self, *args, **kwds) 838 # Lifting succeeded, so variables are initialized and we can run the 839 # stateless function. --&gt; 840 return self._stateless_fn(*args, **kwds) 841 else: 842 canon_args, canon_kwds = ~ anaconda3 envs blog-env lib site-packages tensorflow python eager function.py in __call__(self, *args, **kwargs) 2826 &#34;&#34;&#34;Calls a graph function specialized to the inputs.&#34;&#34;&#34; 2827 with self._lock: -&gt; 2828 graph_function, args, kwargs = self._maybe_define_function(args, kwargs) 2829 return graph_function._filtered_call(args, kwargs) # pylint: disable=protected-access 2830 ~ anaconda3 envs blog-env lib site-packages tensorflow python eager function.py in _maybe_define_function(self, args, kwargs) 3211 3212 self._function_cache.missed.add(call_context_key) -&gt; 3213 graph_function = self._create_graph_function(args, kwargs) 3214 self._function_cache.primary[cache_key] = graph_function 3215 return graph_function, args, kwargs ~ anaconda3 envs blog-env lib site-packages tensorflow python eager function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes) 3073 arg_names=arg_names, 3074 override_flat_arg_shapes=override_flat_arg_shapes, -&gt; 3075 capture_by_value=self._capture_by_value), 3076 self._function_attributes, 3077 function_spec=self.function_spec, ~ anaconda3 envs blog-env lib site-packages tensorflow python framework func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes) 984 _, original_func = tf_decorator.unwrap(python_func) 985 --&gt; 986 func_outputs = python_func(*func_args, **func_kwargs) 987 988 # invariant: `func_outputs` contains only Tensors, CompositeTensors, ~ anaconda3 envs blog-env lib site-packages tensorflow python eager def_function.py in wrapped_fn(*args, **kwds) 598 # __wrapped__ allows AutoGraph to swap in a converted function. We give 599 # the function a weak reference to itself to avoid a reference cycle. --&gt; 600 return weak_wrapped_fn().__wrapped__(*args, **kwds) 601 weak_wrapped_fn = weakref.ref(wrapped_fn) 602 ~ anaconda3 envs blog-env lib site-packages tensorflow python framework func_graph.py in wrapper(*args, **kwargs) 967 recursive=True, 968 optional_features=autograph_options, --&gt; 969 user_requested=True, 970 )) 971 except Exception as e: # pylint:disable=broad-except ~ anaconda3 envs blog-env lib site-packages tensorflow python autograph impl api.py in converted_call(f, args, kwargs, caller_fn_scope, options) 594 try: 595 if kwargs is not None: --&gt; 596 result = converted_f(*effective_args, **kwargs) 597 else: 598 result = converted_f(*effective_args) ~ anaconda3 envs blog-env lib site-packages tensorflow python keras engine training.py in tf__train_function(iterator) 14 try: 15 do_return = True &gt; 16 retval_ = ag__.converted_call(ag__.ld(step_function), (ag__.ld(self), ag__.ld(iterator)), None, fscope) 17 except: 18 do_return = False ~ anaconda3 envs blog-env lib site-packages tensorflow python autograph impl api.py in converted_call(f, args, kwargs, caller_fn_scope, options) 455 if conversion.is_in_whitelist_cache(f, options): 456 logging.log(2, &#39;Whitelisted %s: from cache&#39;, f) --&gt; 457 return _call_unconverted(f, args, kwargs, options, False) 458 459 if ag_ctx.control_status_ctx().status == ag_ctx.Status.DISABLED: ~ anaconda3 envs blog-env lib site-packages tensorflow python autograph impl api.py in _call_unconverted(f, args, kwargs, options, update_cache) 338 if kwargs is not None: 339 return f(*args, **kwargs) --&gt; 340 return f(*args) 341 342 ~ anaconda3 envs blog-env lib site-packages tensorflow python keras engine training.py in step_function(model, iterator) 794 795 data = next(iterator) --&gt; 796 outputs = model.distribute_strategy.run(run_step, args=(data,)) 797 outputs = reduce_per_replica( 798 outputs, self.distribute_strategy, reduction=&#39;first&#39;) ~ anaconda3 envs blog-env lib site-packages tensorflow python distribute distribute_lib.py in run(***failed resolving arguments***) 1209 fn = autograph.tf_convert( 1210 fn, autograph_ctx.control_status_ctx(), convert_by_default=False) -&gt; 1211 return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs) 1212 1213 # TODO(b/151224785): Remove deprecated alias. ~ anaconda3 envs blog-env lib site-packages tensorflow python distribute distribute_lib.py in call_for_each_replica(self, fn, args, kwargs) 2583 kwargs = {} 2584 with self._container_strategy().scope(): -&gt; 2585 return self._call_for_each_replica(fn, args, kwargs) 2586 2587 def _call_for_each_replica(self, fn, args, kwargs): ~ anaconda3 envs blog-env lib site-packages tensorflow python distribute distribute_lib.py in _call_for_each_replica(self, fn, args, kwargs) 2943 self._container_strategy(), 2944 replica_id_in_sync_group=constant_op.constant(0, dtypes.int32)): -&gt; 2945 return fn(*args, **kwargs) 2946 2947 def _reduce_to(self, reduce_op, value, destinations, experimental_hints): ~ anaconda3 envs blog-env lib site-packages tensorflow python autograph impl api.py in wrapper(*args, **kwargs) 253 try: 254 with conversion_ctx: --&gt; 255 return converted_call(f, args, kwargs, options=options) 256 except Exception as e: # pylint:disable=broad-except 257 if hasattr(e, &#39;ag_error_metadata&#39;): ~ anaconda3 envs blog-env lib site-packages tensorflow python autograph impl api.py in converted_call(f, args, kwargs, caller_fn_scope, options) 530 531 if not options.user_requested and conversion.is_whitelisted(f): --&gt; 532 return _call_unconverted(f, args, kwargs, options) 533 534 # internal_convert_user_code is for example turned off when issuing a dynamic ~ anaconda3 envs blog-env lib site-packages tensorflow python autograph impl api.py in _call_unconverted(f, args, kwargs, options, update_cache) 337 338 if kwargs is not None: --&gt; 339 return f(*args, **kwargs) 340 return f(*args) 341 ~ anaconda3 envs blog-env lib site-packages tensorflow python keras engine training.py in run_step(data) 787 788 def run_step(data): --&gt; 789 outputs = model.train_step(data) 790 # Ensure counter is updated only if `train_step` succeeds. 791 with ops.control_dependencies(_minimum_control_deps(outputs)): ~ anaconda3 envs blog-env lib site-packages tensorflow python keras engine training.py in train_step(self, data) 745 746 with backprop.GradientTape() as tape: --&gt; 747 y_pred = self(x, training=True) 748 loss = self.compiled_loss( 749 y, y_pred, sample_weight, regularization_losses=self.losses) ~ anaconda3 envs blog-env lib site-packages tensorflow python keras engine base_layer.py in __call__(self, *args, **kwargs) 983 984 with ops.enable_auto_cast_variables(self._compute_dtype_object): --&gt; 985 outputs = call_fn(inputs, *args, **kwargs) 986 987 if self._activity_regularizer: ~ anaconda3 envs blog-env lib site-packages tensorflow python keras engine sequential.py in call(self, inputs, training, mask) 370 if not self.built: 371 self._init_graph_network(self.inputs, self.outputs) --&gt; 372 return super(Sequential, self).call(inputs, training=training, mask=mask) 373 374 outputs = inputs # handle the corner case where self.layers is empty ~ anaconda3 envs blog-env lib site-packages tensorflow python keras engine functional.py in call(self, inputs, training, mask) 384 &#34;&#34;&#34; 385 return self._run_internal_graph( --&gt; 386 inputs, training=training, mask=mask) 387 388 def compute_output_shape(self, input_shape): ~ anaconda3 envs blog-env lib site-packages tensorflow python keras engine functional.py in _run_internal_graph(self, inputs, training, mask) 506 507 args, kwargs = node.map_arguments(tensor_dict) --&gt; 508 outputs = node.layer(*args, **kwargs) 509 510 # Update tensor_dict. ~ anaconda3 envs blog-env lib site-packages tensorflow python keras layers wrappers.py in __call__(self, inputs, initial_state, constants, **kwargs) 528 529 if initial_state is None and constants is None: --&gt; 530 return super(Bidirectional, self).__call__(inputs, **kwargs) 531 532 # Applies the same workaround as in `RNN.__call__` ~ anaconda3 envs blog-env lib site-packages tensorflow python keras engine base_layer.py in __call__(self, *args, **kwargs) 983 984 with ops.enable_auto_cast_variables(self._compute_dtype_object): --&gt; 985 outputs = call_fn(inputs, *args, **kwargs) 986 987 if self._activity_regularizer: ~ anaconda3 envs blog-env lib site-packages tensorflow python keras layers wrappers.py in call(self, inputs, training, mask, initial_state, constants) 642 643 y = self.forward_layer(forward_inputs, --&gt; 644 initial_state=forward_state, **kwargs) 645 y_rev = self.backward_layer(backward_inputs, 646 initial_state=backward_state, **kwargs) ~ anaconda3 envs blog-env lib site-packages tensorflow python keras layers recurrent.py in __call__(self, inputs, initial_state, constants, **kwargs) 661 662 if initial_state is None and constants is None: --&gt; 663 return super(RNN, self).__call__(inputs, **kwargs) 664 665 # If any of `initial_state` or `constants` are specified and are Keras ~ anaconda3 envs blog-env lib site-packages tensorflow python keras engine base_layer.py in __call__(self, *args, **kwargs) 983 984 with ops.enable_auto_cast_variables(self._compute_dtype_object): --&gt; 985 outputs = call_fn(inputs, *args, **kwargs) 986 987 if self._activity_regularizer: ~ anaconda3 envs blog-env lib site-packages tensorflow python keras layers recurrent_v2.py in call(self, inputs, mask, training, initial_state) 1181 else: 1182 (last_output, outputs, new_h, new_c, -&gt; 1183 runtime) = lstm_with_backend_selection(**normal_lstm_kwargs) 1184 1185 states = [new_h, new_c] ~ anaconda3 envs blog-env lib site-packages tensorflow python keras layers recurrent_v2.py in lstm_with_backend_selection(inputs, init_h, init_c, kernel, recurrent_kernel, bias, mask, time_major, go_backwards, sequence_lengths, zero_output_for_mask) 1557 last_output, outputs, new_h, new_c, runtime = defun_standard_lstm( 1558 **params) -&gt; 1559 function.register(defun_gpu_lstm, **params) 1560 1561 return last_output, outputs, new_h, new_c, runtime ~ anaconda3 envs blog-env lib site-packages tensorflow python eager function.py in register(func, *args, **kwargs) 3239 concrete_func = func.get_concrete_function(*args, **kwargs) 3240 concrete_func.add_to_graph() -&gt; 3241 concrete_func.add_gradient_functions_to_graph() 3242 return concrete_func 3243 ~ anaconda3 envs blog-env lib site-packages tensorflow python eager function.py in add_gradient_functions_to_graph(self, g) 2061 self._delayed_rewrite_functions.forward().add_to_graph(g) 2062 forward_function, backward_function = ( -&gt; 2063 self._delayed_rewrite_functions.forward_backward()) 2064 forward_function.add_to_graph(g) 2065 backward_function.add_to_graph(g) ~ anaconda3 envs blog-env lib site-packages tensorflow python eager function.py in forward_backward(self, num_doutputs) 619 if forward_backward is not None: 620 return forward_backward --&gt; 621 forward, backward = self._construct_forward_backward(num_doutputs) 622 self._cached_function_pairs[num_doutputs] = (forward, backward) 623 return forward, backward ~ anaconda3 envs blog-env lib site-packages tensorflow python eager function.py in _construct_forward_backward(self, num_doutputs) 667 args=[], kwargs={}, 668 signature=signature, --&gt; 669 func_graph=backwards_graph) 670 backwards_graph_captures = backwards_graph.external_captures 671 captures_from_forward = [ ~ anaconda3 envs blog-env lib site-packages tensorflow python framework func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes) 984 _, original_func = tf_decorator.unwrap(python_func) 985 --&gt; 986 func_outputs = python_func(*func_args, **func_kwargs) 987 988 # invariant: `func_outputs` contains only Tensors, CompositeTensors, ~ anaconda3 envs blog-env lib site-packages tensorflow python eager function.py in _backprop_function(*grad_ys) 657 self._func_graph.inputs, 658 grad_ys=grad_ys, --&gt; 659 src_graph=self._func_graph) 660 661 with self._func_graph.as_default(): ~ anaconda3 envs blog-env lib site-packages tensorflow python ops gradients_util.py in _GradientsHelper(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph) 667 # functions. 668 in_grads = _MaybeCompile(grad_scope, op, func_call, --&gt; 669 lambda: grad_fn(op, *out_grads)) 670 else: 671 # For function call ops, we add a &#39;SymbolicGradient&#39; ~ anaconda3 envs blog-env lib site-packages tensorflow python ops gradients_util.py in _MaybeCompile(scope, op, func, grad_fn) 334 xla_scope = op.get_attr(&#34;_XlaScope&#34;).decode() 335 except ValueError: --&gt; 336 return grad_fn() # Exit early 337 338 if not xla_compile: ~ anaconda3 envs blog-env lib site-packages tensorflow python ops gradients_util.py in &lt;lambda&gt;() 667 # functions. 668 in_grads = _MaybeCompile(grad_scope, op, func_call, --&gt; 669 lambda: grad_fn(op, *out_grads)) 670 else: 671 # For function call ops, we add a &#39;SymbolicGradient&#39; ~ anaconda3 envs blog-env lib site-packages tensorflow python ops array_grad.py in _SplitGrad(op, *grads) 317 @ops.RegisterGradient(&#34;Split&#34;) 318 def _SplitGrad(op, *grads): --&gt; 319 return None, array_ops.concat(list(grads), op.inputs[0]) 320 321 ~ anaconda3 envs blog-env lib site-packages tensorflow python util dispatch.py in wrapper(*args, **kwargs) 199 &#34;&#34;&#34;Call target, and fall back on dispatchers if there is a TypeError.&#34;&#34;&#34; 200 try: --&gt; 201 return target(*args, **kwargs) 202 except (TypeError, ValueError): 203 # Note: convert_to_eager_tensor currently raises a ValueError, not a ~ anaconda3 envs blog-env lib site-packages tensorflow python ops array_ops.py in concat(values, axis, name) 1652 dtype=dtypes.int32).get_shape().assert_has_rank(0) 1653 return identity(values[0], name=name) -&gt; 1654 return gen_array_ops.concat_v2(values=values, axis=axis, name=name) 1655 1656 ~ anaconda3 envs blog-env lib site-packages tensorflow python ops gen_array_ops.py in concat_v2(values, axis, name) 1219 _attr_N = len(values) 1220 _, _, _op, _outputs = _op_def_library._apply_op_helper( -&gt; 1221 &#34;ConcatV2&#34;, values=values, axis=axis, name=name) 1222 _result = _outputs[:] 1223 if _execute.must_record_gradient(): ~ anaconda3 envs blog-env lib site-packages tensorflow python framework op_def_library.py in _apply_op_helper(op_type_name, name, **keywords) 742 op = g._create_op_internal(op_type_name, inputs, dtypes=None, 743 name=scope, input_types=input_types, --&gt; 744 attrs=attr_protos, op_def=op_def) 745 746 # `outputs` is returned as a separate return value so that the output ~ anaconda3 envs blog-env lib site-packages tensorflow python framework func_graph.py in _create_op_internal(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device) 587 if ctxt is not None and hasattr(ctxt, &#34;AddValue&#34;): 588 inp = ctxt.AddValue(inp) --&gt; 589 inp = self.capture(inp) 590 inputs[i] = inp 591 return super(FuncGraph, self)._create_op_internal( # pylint: disable=protected-access ~ anaconda3 envs blog-env lib site-packages tensorflow python framework func_graph.py in capture(self, tensor, name, shape) 639 % (tensor, tensor.graph, self)) 640 inner_graph = inner_graph.outer_graph --&gt; 641 return self._capture_helper(tensor, name) 642 return tensor 643 ~ anaconda3 envs blog-env lib site-packages tensorflow python framework func_graph.py in _capture_helper(self, tensor, name, shape) 646 if capture is None: 647 placeholder = _create_substitute_placeholder( --&gt; 648 tensor, name=name, dtype=tensor.dtype, shape=shape) 649 # Record the composite device as an attribute to the placeholder. 650 # This attribute would be propogated into the arg_attr of the FunctionDef. ~ anaconda3 envs blog-env lib site-packages tensorflow python framework func_graph.py in _create_substitute_placeholder(value, name, dtype, shape) 1129 with ops.control_dependencies(None): 1130 placeholder = graph_placeholder( -&gt; 1131 dtype=dtype or value.dtype, shape=shape, name=name) 1132 custom_gradient.copy_handle_data(value, placeholder) 1133 return placeholder ~ anaconda3 envs blog-env lib site-packages tensorflow python eager graph_only_ops.py in graph_placeholder(dtype, shape, name) 38 op = g._create_op_internal( # pylint: disable=protected-access 39 &#34;Placeholder&#34;, [], [dtype], input_types=[], &gt; 40 attrs=attrs, name=name) 41 result, = op.outputs 42 if op_callbacks.should_invoke_op_callbacks(): ~ anaconda3 envs blog-env lib site-packages tensorflow python framework func_graph.py in _create_op_internal(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device) 591 return super(FuncGraph, self)._create_op_internal( # pylint: disable=protected-access 592 op_type, inputs, dtypes, input_types, name, attrs, op_def, --&gt; 593 compute_device) 594 595 def capture(self, tensor, name=None, shape=None): ~ anaconda3 envs blog-env lib site-packages tensorflow python framework ops.py in _create_op_internal(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device) 3483 input_types=input_types, 3484 original_op=self._default_original_op, -&gt; 3485 op_def=op_def) 3486 self._create_op_helper(ret, compute_device=compute_device) 3487 return ret ~ anaconda3 envs blog-env lib site-packages tensorflow python framework ops.py in __init__(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def) 1973 op_def = self._graph._get_op_def(node_def.op) 1974 self._c_op = _create_c_op(self._graph, node_def, inputs, -&gt; 1975 control_input_ops, op_def) 1976 name = compat.as_str(node_def.name) 1977 # pylint: enable=protected-access ~ anaconda3 envs blog-env lib site-packages tensorflow python framework ops.py in _create_c_op(graph, node_def, inputs, control_inputs, op_def) 1810 1811 try: -&gt; 1812 c_op = pywrap_tf_session.TF_FinishOperation(op_desc) 1813 except errors.InvalidArgumentError as e: 1814 # Convert to ValueError for backwards compatibility. KeyboardInterrupt: . We can print out the results summary: . tuner.results_summary(num_trials=1) . And we can retrieve the best hyperparameters configuration: . best_params = tuner.get_best_hyperparameters(num_trials=1)[0] . Which we can use to build the model with the best hyperparameters configuration: . model = tuner.hypermodel.build(best_params) . Here is a description of the model&#39;s architecture: . model.summary( line_length=120) . Let&#39;s fit this model using the training data used during the search: . history = model.fit( x=X_train, y=y_train, shuffle=True, batch_size=params[&#39;batch_size&#39;], validation_data=(X_test, y_test), epochs=params[&#39;max_epochs&#39;], callbacks=[ kc.EarlyStopping( monitor=&#39;val_loss&#39;, patience=params[&#39;patience&#39;], verbose=1, mode=&#39;min&#39;, restore_best_weights=True), kc.TerminateOnNaN() ], verbose=2) . It is a good time to save our model for future use: . model.save( &#39;./models/{0}.h5&#39;.format( params[&#39;experiment_name&#39;])) .",
            "url": "https://braibaud.github.io/blog/lstm/keras/keras%20tuner/python/machine%20learning/timeseries/2021/05/31/LSTM-timeseries-forecasting-with-Keras-Tuner.html",
            "relUrl": "/lstm/keras/keras%20tuner/python/machine%20learning/timeseries/2021/05/31/LSTM-timeseries-forecasting-with-Keras-Tuner.html",
            "date": " • May 31, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi there 👋 . I am currently working as Data Science Lead EMEA in Geneva, Switzerland. . My Software Projects . Related to Data Science: . PWML: Python 3 library useful for training hierarchical classification models on top of scikit-learn. It also contains a tons of useful helpers for both results representation as well as experiments management using Neptune. Deployed on Pypi. | . | Related to Space Exploration: . Nasa Rover Images: A Python project for easing rover images download and images post-processing (e.g. debayering). This is also the place where I post stitched and post-processed images (Python 3 + Jupyter based). | . | Related to Home-Assistant: . AirThings Integration: Home Assistant custom component for the AirThings platform integration (Python 3 based). | AirThings API: Python Wrappers for AirThings public dashboard API (Python 3 based). Deployed on PyPi. | . | Other Projects: . ParallelExecution: A SQL Server solution for parallelizing SQL Server commands (.NET C# based). | . | . My Statistics . .",
          "url": "https://braibaud.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://braibaud.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}